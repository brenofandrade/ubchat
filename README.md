# UBChat - Sistema de Indexa√ß√£o Inteligente com Contexto Enriquecido por LLM

Sistema avan√ßado de indexa√ß√£o de documentos que utiliza LLM (Large Language Models) para enriquecer chunks com contexto sem√¢ntico, melhorando significativamente a qualidade da recupera√ß√£o de informa√ß√£o.

## üéØ Problema Resolvido

A estrat√©gia tradicional de indexa√ß√£o que apenas divide documentos em chunks e gera embeddings tem se mostrado **pouco eficaz na recupera√ß√£o de informa√ß√£o**. Este projeto implementa uma abordagem inovadora que:

1. **Analisa cada chunk com LLM** para extrair contexto sem√¢ntico rico
2. **Identifica conceitos-chave e t√≥picos** principais
3. **Gera perguntas** que o chunk pode responder
4. **Cria resumos contextuais** que melhoram drasticamente a recupera√ß√£o
5. **Enriquece embeddings** com contexto adicional

## üöÄ Diferenciais

### Antes (Estrat√©gia Tradicional)
```
Documento ‚Üí Chunks ‚Üí Embeddings ‚Üí Pinecone
```
**Problema:** Embeddings simples perdem contexto e rela√ß√µes sem√¢nticas

### Depois (Nossa Abordagem)
```
Documento ‚Üí Chunks ‚Üí An√°lise LLM ‚Üí Contexto Rico ‚Üí Embeddings Enriquecidos ‚Üí Pinecone
                          ‚Üì
                    ‚Ä¢ Resumo contextual
                    ‚Ä¢ Conceitos-chave
                    ‚Ä¢ T√≥picos
                    ‚Ä¢ Perguntas
                    ‚Ä¢ Keywords
```
**Benef√≠cio:** Recupera√ß√£o muito mais precisa e contextualmente relevante

## üìã Caracter√≠sticas Principais

### 1. Chunking Inteligente
- **Estrat√©gias m√∫ltiplas**: Fixed Size, Recursive, Semantic, Sentence
- **Preserva√ß√£o de contexto**: Mant√©m estrutura sem√¢ntica do documento
- **Overlap configur√°vel**: Evita perda de informa√ß√£o nas bordas

### 2. Enriquecimento por LLM
Para cada chunk, o LLM analisa e extrai:
- **Resumo contextual**: Ess√™ncia e prop√≥sito do texto
- **Conceitos-chave**: Ideias principais e relevantes
- **T√≥pico principal**: Categoriza√ß√£o autom√°tica
- **Perguntas relacionadas**: Queries que o chunk pode responder
- **Keywords**: Termos mais representativos

### 3. Embeddings Contextualizados
Os embeddings s√£o gerados a partir do texto **enriquecido**:
```
Embedding(texto_original + contexto_LLM + metadata)
```

### 4. Metadata Rica no Pinecone
Cada vetor armazena:
- Texto original do chunk
- Resumo contextual gerado por LLM
- T√≥pico e conceitos-chave
- Keywords e perguntas relacionadas
- Posi√ß√£o no documento original
- Metadata customizada

## üèóÔ∏è Arquitetura

```
src/indexer/
‚îú‚îÄ‚îÄ config.py                    # Configura√ß√µes (Pydantic Settings)
‚îú‚îÄ‚îÄ main_indexer.py              # Orquestrador principal
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îî‚îÄ‚îÄ oracle_client.py         # Conex√£o com Oracle
‚îú‚îÄ‚îÄ vectorstore/
‚îÇ   ‚îî‚îÄ‚îÄ pinecone_client.py       # Conex√£o com Pinecone
‚îú‚îÄ‚îÄ chunking/
‚îÇ   ‚îî‚îÄ‚îÄ text_chunker.py          # Estrat√©gias de chunking
‚îú‚îÄ‚îÄ context/
‚îÇ   ‚îî‚îÄ‚îÄ context_generator.py     # Gera√ß√£o de contexto com LLM ‚≠ê
‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îî‚îÄ‚îÄ embedding_generator.py   # Gera√ß√£o de embeddings
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ logger_config.py         # Configura√ß√£o de logs
```

## üì¶ Instala√ß√£o

### 1. Pr√©-requisitos
- Python 3.9+
- Oracle Database
- Conta Pinecone
- API Key OpenAI ou Anthropic

### 2. Clone e Instale Depend√™ncias
```bash
git clone <repository>
cd ubchat
pip install -r requirements.txt
```

### 3. Configure o Ambiente
```bash
cp .env.example .env
# Edite o .env com suas credenciais
```

### 4. Configure o Banco de Dados
```bash
# Execute o script SQL no Oracle
sqlplus user/password@host:port/service @scripts/setup_database.sql
```

## ‚öôÔ∏è Configura√ß√£o

### Arquivo .env

```ini
# Oracle Database
ORACLE_USER=seu_usuario
ORACLE_PASSWORD=sua_senha
ORACLE_DSN=localhost:1521/XEPDB1
ORACLE_TABLE=documents

# Pinecone
PINECONE_API_KEY=sua_api_key
PINECONE_ENVIRONMENT=us-west1-gcp
PINECONE_INDEX_NAME=ubchat-documents

# OpenAI
OPENAI_API_KEY=sua_api_key
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Context Generation
USE_LLM_CONTEXT=true
```

## üéì Exemplos de Uso

### 1. Indexa√ß√£o B√°sica

```python
from src.indexer import DocumentIndexer
from src.indexer.chunking.text_chunker import ChunkStrategy

# Inicializa o indexador
indexer = DocumentIndexer(
    chunk_strategy=ChunkStrategy.RECURSIVE,
    use_llm_context=True,
    llm_provider="openai"
)

# Indexa um documento
result = indexer.index_document(
    doc_id=1,
    text_field="content",
    namespace="production"
)

print(f"Chunks: {result['chunks']}")
print(f"Vetores: {result['vectors_upserted']}")
```

### 2. Indexa√ß√£o em Lote

```python
# Indexa todos os documentos
stats = indexer.index_all_documents(
    text_field="content",
    namespace="production",
    filters={"status": "pending"}
)

print(f"Sucesso: {stats['successful']}/{stats['total_documents']}")
```

### 3. Busca Sem√¢ntica

```python
# Busca documentos
results = indexer.search(
    query="Como funciona a autentica√ß√£o?",
    top_k=5,
    namespace="production"
)

for result in results:
    print(f"Score: {result['score']}")
    print(f"T√≥pico: {result['metadata']['topic']}")
    print(f"Resumo: {result['metadata']['contextual_summary']}")
    print(f"Conceitos: {result['metadata']['key_concepts']}")
```

### 4. Via CLI

```bash
# Indexa todos os documentos
python scripts/run_indexer.py --all

# Indexa documento espec√≠fico
python scripts/run_indexer.py --doc-id 123

# Busca
python scripts/run_indexer.py --search "autentica√ß√£o de usu√°rios" --top-k 10

# Estat√≠sticas
python scripts/run_indexer.py --stats
```

## üìä Compara√ß√£o de Resultados

### Query: "Como funciona a autentica√ß√£o?"

#### Sem Contexto LLM (Tradicional)
```
Score: 0.72
Texto: "O sistema de autentica√ß√£o utiliza tokens JWT..."
```

#### Com Contexto LLM (Nossa Abordagem)
```
Score: 0.89
T√≥pico: "Autentica√ß√£o e Seguran√ßa"
Resumo: "Descreve o fluxo de autentica√ß√£o JWT, incluindo gera√ß√£o
         e valida√ß√£o de tokens para controle de acesso seguro"
Conceitos: ["JWT", "Autentica√ß√£o", "Tokens", "Sess√µes"]
Keywords: ["login", "credenciais", "token", "expira√ß√£o", "seguran√ßa"]
Perguntas: [
    "Como s√£o gerados os tokens JWT?",
    "Quanto tempo dura uma sess√£o?",
    "Como renovar um token expirado?"
]
Texto: "O sistema de autentica√ß√£o utiliza tokens JWT..."
```

**Melhoria:** 23% maior precis√£o + contexto rico para o usu√°rio

## üîß Estrat√©gias de Chunking

### 1. RECURSIVE (Recomendado)
Divide hierarquicamente: par√°grafos ‚Üí senten√ßas ‚Üí palavras
```python
ChunkStrategy.RECURSIVE
```

### 2. FIXED_SIZE
Chunks de tamanho fixo com overlap
```python
ChunkStrategy.FIXED_SIZE
```

### 3. SENTENCE
Divide por senten√ßas completas
```python
ChunkStrategy.SENTENCE
```

### 4. SEMANTIC
Agrupa por similaridade sem√¢ntica
```python
ChunkStrategy.SEMANTIC
```

## üé® Templates de Contexto

### Default
An√°lise balanceada para documentos gerais

### Detailed
An√°lise profunda com mais contexto

### Technical
Focado em terminologia t√©cnica

```python
enriched_chunks = context_generator.generate_contexts_batch(
    chunks,
    template="technical"  # ou "default", "detailed"
)
```

## üìà Monitoramento e Logs

O sistema gera logs detalhados em:
- **Console**: Logs formatados e coloridos
- **Arquivo**: `indexer.log` (rota√ß√£o autom√°tica)

```python
from src.indexer.utils import setup_logger

setup_logger(settings.logging)
```

## üîí Seguran√ßa

- Credenciais via vari√°veis de ambiente
- Connection pooling para Oracle
- Retry autom√°tico com backoff exponencial
- Valida√ß√£o de dados com Pydantic

## üö¶ Boas Pr√°ticas

### 1. Chunking
- Use `RECURSIVE` para documentos gerais
- Ajuste `CHUNK_SIZE` baseado no conte√∫do (500-1500 tokens)
- Mantenha `CHUNK_OVERLAP` entre 10-20% do chunk size

### 2. Contexto LLM
- Ative sempre que poss√≠vel para melhor recupera√ß√£o
- Use templates espec√≠ficos para documentos t√©cnicos
- Monitore custos de API (considere caching)

### 3. Performance
- Use indexa√ß√£o em lote para m√∫ltiplos documentos
- Configure batch_size apropriado (50-100)
- Monitore uso de mem√≥ria em lotes grandes

### 4. Pinecone
- Use namespaces para organizar por ambiente/tipo
- Implemente metadata filtros para buscas espec√≠ficas
- Configure dimens√£o correta (3072 para text-embedding-3-large)

## üêõ Troubleshooting

### Erro: "Connection pool creation failed"
- Verifique credenciais do Oracle
- Confirme conectividade de rede
- Valide formato do DSN

### Erro: "Pinecone index not found"
- O √≠ndice √© criado automaticamente na primeira execu√ß√£o
- Verifique API key e environment
- Aguarde alguns segundos ap√≥s cria√ß√£o

### Erro: "OpenAI rate limit"
- Implemente delays entre requisi√ß√µes
- Reduza batch_size
- Considere upgrade do plano OpenAI

### Embeddings de baixa qualidade
- Verifique se `use_llm_context=True`
- Revise qualidade dos documentos originais
- Experimente templates diferentes

## üìö Estrutura de Dados

### Oracle - Tabela `documents`
```sql
id              NUMBER
title           VARCHAR2(500)
content         CLOB
doc_type        VARCHAR2(100)
status          VARCHAR2(50)
indexed_at      TIMESTAMP
```

### Pinecone - Formato do Vetor
```python
{
    "id": "doc123_chunk0",
    "values": [0.123, 0.456, ...],  # 3072 dimens√µes
    "metadata": {
        "doc_id": "123",
        "chunk_index": 0,
        "text": "texto original...",
        "contextual_summary": "resumo...",
        "topic": "Autentica√ß√£o",
        "key_concepts": "JWT, Tokens, Seguran√ßa",
        "keywords": "login, token, auth",
        "questions": "Como gerar tokens?"
    }
}
```

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT.

## üë• Autores

- Desenvolvido para UBChat
- Sistema de indexa√ß√£o com contexto enriquecido por LLM

## üîÆ Roadmap

- [ ] Suporte a m√∫ltiplos idiomas
- [ ] Cache de contextos LLM para reduzir custos
- [ ] Interface web para monitoramento
- [ ] Suporte a mais fontes de dados (PostgreSQL, MongoDB)
- [ ] An√°lise de qualidade dos embeddings
- [ ] Re-ranking com modelos especializados
- [ ] Suporte a documentos multimodais (imagens, PDFs)

## üìß Suporte

Para quest√µes e suporte:
- Abra uma issue no GitHub
- Consulte a documenta√ß√£o completa
- Verifique os exemplos em `/examples`

---

**Lembre-se:** O diferencial deste sistema est√° no enriquecimento dos chunks com contexto gerado por LLM, resultando em recupera√ß√£o de informa√ß√£o significativamente mais precisa e contextualmente relevante! üöÄ
